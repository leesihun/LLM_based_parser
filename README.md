# HE Team LLM Assistant

A multi-modal AI assistant system built with Flask, featuring RAG (Retrieval Augmented Generation), web search capabilities, and comprehensive file handling.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Project Structure](#project-structure)
- [Development](#development)
- [API Documentation](#api-documentation)
- [Testing](#testing)
- [Deployment](#deployment)

## Overview

The HE Team LLM Assistant is a comprehensive AI-powered assistant system designed to provide intelligent responses using multiple data sources:

- **Local LLM Integration** via Ollama
- **RAG System** with ChromaDB for knowledge base queries
- **Web Search** with intelligent keyword extraction
- **File Analysis** supporting multiple formats
- **Multi-user Support** with role-based access control
- **Session Management** with conversation history

## Features

### Core Capabilities

- **Chat Interface**: Natural language conversation with context retention
- **Knowledge Base Search**: Semantic search through embedded documents using ChromaDB
- **Web Search Integration**: Real-time web search with multiple providers (DuckDuckGo, Bing, Brave)
- **File Processing**: Upload and analyze documents (PDF, DOCX, XLSX, TXT, code files)
- **User Management**: Admin panel for user creation and management
- **Multi-Session Support**: Maintain multiple concurrent conversations

### Technical Features

- **Streaming Responses**: Real-time LLM output streaming
- **Advanced Keyword Extraction**: LLM-assisted keyword extraction for better search results
- **Result Filtering**: Relevance scoring, duplicate detection, domain filtering
- **Caching System**: Configurable caching for search results and embeddings
- **Analytics**: Search analytics and usage tracking
- **TypeScript Bridge**: Integration with TypeScript-based web search module

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flask Application                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Routes: Auth, Chat, Search, RAG, Files, Admin, System  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Core  â”‚      â”‚ Services â”‚      â”‚ External â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚                â”‚                  â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Client   â”‚ â”‚ RAG System  â”‚   â”‚ Ollama  â”‚
â”‚ User Mgmt    â”‚ â”‚ Web Search  â”‚   â”‚ ChromaDBâ”‚
â”‚ Memory       â”‚ â”‚ File Handlerâ”‚   â”‚ Seleniumâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Technologies

- **Backend**: Python 3.8+, Flask 2.3.3
- **LLM**: Ollama (default: gemma3:12b)
- **Vector DB**: ChromaDB 0.4.15 with Ollama embeddings (nomic-embed-text)
- **Web Automation**: Selenium 4.15.2
- **Frontend**: HTML/CSS/JavaScript (served statically)
- **Data Processing**: Pandas 2.0.3 for Excel files

## Quick Start

### Prerequisites

1. **Python 3.8+** installed
2. **Ollama** running locally (download from [ollama.ai](https://ollama.ai))
3. **Chrome/Chromium** browser (for web search)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd LLM_based_parser
```

2. Install Python dependencies:
```bash
pip install -r requirements.txt
```

3. Pull required Ollama models:
```bash
ollama pull gemma3:12b
ollama pull nomic-embed-text
```

4. Initialize the system (process Excel data and set up RAG):
```bash
python setup_system.py
```

5. Start the server:
```bash
python server.py
```

The application will be available at:
- Local: http://localhost:8000
- Network: http://<your-ip>:8000

### Default Credentials

- **Admin**: username: `admin`, password: `admin123`
- **Guest**: username: `guest`, password: `guest_test1`

## Configuration

The system uses a single configuration file: `config.json` at the root directory.

### Key Configuration Sections

#### Server Settings
```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 8000
  }
}
```

#### Ollama Configuration
```json
{
  "ollama": {
    "host": "http://localhost:11434",
    "model": "gemma3:12b",
    "temperature": 0.7,
    "num_ctx": 8192
  }
}
```

#### RAG System
```json
{
  "rag": {
    "embedding": {
      "provider": "ollama",
      "model": "nomic-embed-text:latest"
    },
    "chunking": {
      "strategy": "semantic",
      "chunk_size": 1000,
      "overlap": 200
    },
    "search": {
      "similarity_threshold": 0.8
    }
  }
}
```

#### Web Search
```json
{
  "web_search": {
    "enabled": true,
    "default_provider": "duckduckgo",
    "use_keyword_extraction": true,
    "max_results": 5
  }
}
```

See the full configuration file for all available options.

## Project Structure

```
LLM_based_parser/
â”œâ”€â”€ config.json                 # Main configuration file
â”œâ”€â”€ server.py                   # Application entry point
â”œâ”€â”€ setup_system.py            # System initialization script
â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py       # Flask app factory
â”‚   â”‚   â”œâ”€â”€ container.py      # Dependency injection
â”‚   â”‚   â”œâ”€â”€ errors.py         # Error handlers
â”‚   â”‚   â””â”€â”€ routes/           # API route blueprints
â”‚   â”‚       â”œâ”€â”€ auth.py       # Authentication endpoints
â”‚   â”‚       â”œâ”€â”€ chat.py       # Chat/conversation endpoints
â”‚   â”‚       â”œâ”€â”€ search.py     # Web search endpoints
â”‚   â”‚       â”œâ”€â”€ rag.py        # RAG system endpoints
â”‚   â”‚       â”œâ”€â”€ files.py      # File management endpoints
â”‚   â”‚       â”œâ”€â”€ admin.py      # Admin/user management
â”‚   â”‚       â”œâ”€â”€ system.py     # System health/info
â”‚   â”‚       â””â”€â”€ model_config.py # Model configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llm_client.py     # Ollama LLM client
â”‚   â”‚   â”œâ”€â”€ user_management.py # User authentication
â”‚   â”‚   â””â”€â”€ conversation_memory.py # Session management
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â””â”€â”€ rag_system.py # ChromaDB RAG implementation
â”‚   â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”‚   â”œâ”€â”€ web_search_feature.py # Web search orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ keyword_extractor.py  # Keyword extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ result_filter.py      # Result filtering
â”‚   â”‚   â”‚   â””â”€â”€ typescript_bridge.py  # TS integration
â”‚   â”‚   â””â”€â”€ files/
â”‚   â”‚       â””â”€â”€ file_handler.py # File processing
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ users.json        # User data (passwords, roles)
â”‚       â””â”€â”€ user_sessions.json # Active sessions
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ static/              # HTML/CSS/JS frontend files
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ combined_data.md     # Knowledge base markdown
â”‚   â””â”€â”€ chroma_db/          # ChromaDB persistence
â”‚
â”œâ”€â”€ conversations/           # User conversation history
â”œâ”€â”€ uploads/                # User uploaded files
â””â”€â”€ websearch_ts/           # TypeScript web search module
```

## Development

### Running in Development Mode

```bash
# Start the server with auto-reload
python server.py
```

### Adding New Features

The system follows a modular blueprint pattern:

1. **Create a new service** in `backend/services/`
2. **Register in container** at `backend/app/container.py`
3. **Create route blueprint** in `backend/app/routes/`
4. **Register blueprint** in `backend/app/routes/__init__.py`

### Testing Individual Components

```bash
# Test LLM connection
python -c "from backend.core.llm_client import LLMClient; client = LLMClient('config.json'); print(client.chat_completion([{'role': 'user', 'content': 'Hello'}]))"

# Test RAG system
python -c "from backend.services.rag.rag_system import RAGSystem; rag = RAGSystem('config.json'); print(rag.get_stats())"

# Test web search
python -c "from backend.services.search.web_search_feature import WebSearchFeature; ws = WebSearchFeature({}, None); print(ws.get_search_capabilities())"
```

### Component Testing Commands

```bash
# Keyword extraction
python -c "from backend.services.search.keyword_extractor import KeywordExtractor; ke = KeywordExtractor({}, None); print(ke.extract_keywords('machine learning algorithms'))"

# Health check
curl http://localhost:8000/health

# Authentication test
curl -X POST http://localhost:8000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123"}'
```

## API Documentation

See [API_DOCUMENTATION.md](API_DOCUMENTATION.md) for complete API reference.

### Quick API Overview

- **Authentication**: `/api/auth/login`, `/api/auth/logout`
- **Chat**: `/api/chat/messages` - Send messages with optional RAG
- **Search**: `/api/search/web` - Perform web searches
- **RAG**: `/api/rag/search` - Query knowledge base
- **Files**: `/api/files/upload`, `/api/files/<id>/read`
- **Admin**: `/api/admin/users` - User management (admin only)
- **System**: `/health`, `/api/system/info`

## Testing

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest backend/tests/test_app.py

# Run with coverage
pytest --cov=backend
```

### Manual Testing

Use the included frontend interface or tools like Postman/curl to test API endpoints.

## Deployment

### Production Considerations

1. **Change the secret key** in `config.json`
2. **Use environment variables** for sensitive configuration
3. **Set up HTTPS** with reverse proxy (nginx/Apache)
4. **Configure firewall** rules for ports 8000 and 11434
5. **Use production WSGI server** (gunicorn/uwsgi)

### Production Server Setup

```bash
# Install gunicorn
pip install gunicorn

# Run with gunicorn
gunicorn -w 4 -b 0.0.0.0:8000 server:app
```

### Docker Deployment (Future)

Docker support is planned for future releases.

## System Requirements

### Minimum Requirements
- CPU: 4 cores
- RAM: 8GB (16GB recommended)
- Storage: 10GB free space
- OS: Windows 10+, Linux, macOS

### Network Requirements
- Port 8000: Web server
- Port 11434: Ollama API
- Internet connection for web search features

## Troubleshooting

### Common Issues

**Ollama Connection Failed**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama service
ollama serve
```

**ChromaDB Errors**
```bash
# Reinitialize the database
python setup_system.py
```

**Web Search Not Working**
- Ensure Chrome/Chromium is installed
- Check `web_search.enabled` in config.json
- Verify network connectivity

**Permission Errors**
```bash
# Ensure correct permissions
chmod -R 755 data/ conversations/ uploads/
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

[Specify your license here]

## Support

For issues and questions:
- Create an issue on GitHub
- Contact the HE team

## Acknowledgments

- Ollama for local LLM inference
- ChromaDB for vector database
- The open-source community

## Changelog

### Version 2.1.5 (2025-10-17)

**Improved Chat Output Display:**
- Chat sections now show **only the answer**, not full API response objects
- Updated sections:
  - 3.1: Send a Simple Chat Message - shows answer with session ID
  - 3.2: Continue the Conversation - shows answer only
  - 3.3: Chat with Web Search - shows answer with context note
  - 6.4: Chat with RAG Context - shows answer with context note
  - 7.3.2: Text File Analysis - shows answer for both initial and follow-up questions
- Clean output format with separator lines for better readability
- Full API response shown only on errors for debugging

**File Changes:**
- `python_API_call.ipynb`: Improved output formatting for all chat sections
- `README.md`: Updated changelog

### Version 2.1.4 (2025-10-17)

**JSON Path Queries Fixed (Section 7.5.4):**
- Added automatic `json_file_id` creation if not already defined
  - Checks if variable exists, uploads warpage JSON file if needed
  - Uses `data/warpage_data/20250925_stats.json` as specified
  - Creates sample data if file doesn't exist
- Updated all query examples to use warpage data structure:
  - Example 1: Query specific material analysis (`material_analysis.0`)
  - Example 2: Query warpage statistics section (`warpage_statistics`)
  - Example 3: Query dataset information (`dataset_info`)
- Removed old "users" data references, now fully aligned with warpage data

**File Changes:**
- `python_API_call.ipynb`: Section 7.5.4 now self-contained with warpage data
- `README.md`: Updated changelog

### Version 2.1.3 (2025-10-17)

**RAG Section Reorganization:**
- **Section 6.1** now creates RAG dataset FIRST (moved from 6.5)
  - Emphasizes that RAG dataset must be created before querying
  - Improved output formatting: shows only progress messages during upload
  - Full API response displayed only at the end for verification
  - Cleaner progress indicators: [1/3], [2/3], [3/3] with emojis
- All other section 6 subsections renumbered accordingly (6.1â†’6.2, 6.2â†’6.3, etc.)

**Output Improvements:**
- Progress-based output for RAG dataset creation
- Clear status messages with âœ“, âŒ, ğŸ“¤, ğŸ“Š indicators
- Summary table format for RAG statistics
- Full response shown only once at completion

**File Changes:**
- `python_API_call.ipynb`: RAG section reorganized with improved UX
- `README.md`: Updated changelog

### Version 2.1.2 (2025-10-17)

**API Notebook Improvements:**
- Reorganized JSON processing section (7.5) with proper subsection ordering
- Removed duplicate section headers and subsections
- Added section 6.5: RAG dataset creation from markdown files
  - Demonstrates loading `data/combined_data.md` into the RAG system
  - Shows verification of loaded data via RAG statistics
- Updated JSON file examples to use warpage analysis data
  - Changed from generic user data to `data/warpage_data/20250925_stats.json`
  - Includes sample warpage statistics structure with auto-generation if file missing
- Improved documentation clarity and workflow examples

**File Changes:**
- `python_API_call.ipynb`: Major reorganization and content updates

---

**Version**: 2.1.5
**Last Updated**: 2025-10-17

### Previous Versions

**Version 2.1.1 (2025-10-17)**
- Initial comprehensive API documentation
- Complete endpoint coverage in Jupyter notebook

**Version 1.0.0**
- Initial release with core features
