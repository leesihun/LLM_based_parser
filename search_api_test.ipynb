{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Search API Test Notebook\n",
        "\n",
        "This self-contained notebook tests multiple web search providers and reports latency, status, and top results.\n",
        "\n",
        "Providers included:\n",
        "- Bing Web Search API (Azure Cognitive Services)\n",
        "- SerpAPI (engine=bing)\n",
        "- SearXNG (public or self-hosted)\n",
        "- DuckDuckGo HTML (no-JS HTML endpoint)\n",
        "- Brave HTML\n",
        "\n",
        "Instructions:\n",
        "- Set API keys/endpoints in the Config cell below (or via environment variables).\n",
        "- Run the cells top-to-bottom. The notebook installs missing dependencies at runtime.\n",
        "- Results include a summary table and sample results per provider.\n",
        "\n",
        "Notes:\n",
        "- Use responsibly and comply with each provider's Terms of Service.\n",
        "- HTML providers may change markup; simple parsers here are best-effort.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime setup: imports and on-the-fly installs\n",
        "import sys, subprocess, json, os, time, random\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# Lazy installer\n",
        "\n",
        "def ensure(package: str, import_name: Optional[str] = None):\n",
        "    try:\n",
        "        __import__(import_name or package)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "# Ensure dependencies used by HTML parsers and table rendering\n",
        "for pkg, imp in [\n",
        "    ('requests', 'requests'),\n",
        "    ('beautifulsoup4', 'bs4'),\n",
        "    ('pandas', 'pandas'),\n",
        "    ('tabulate', 'tabulate'),\n",
        "]:\n",
        "    ensure(pkg, imp)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Utility: timing\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start = time.time()\n",
        "        return self\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        self.end = time.time()\n",
        "        self.ms = int((self.end - self.start) * 1000)\n",
        "\n",
        "# Normalize result schema\n",
        "Result = Dict[str, str]\n",
        "\n",
        "\n",
        "def truncate(text: str, n: int = 200) -> str:\n",
        "    if not text:\n",
        "        return ''\n",
        "    return text if len(text) <= n else text[:n] + '...'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config: API keys and endpoints\n",
        "CONFIG = {\n",
        "    # Bing Web Search API (Azure)\n",
        "    # Docs: https://learn.microsoft.com/azure/cognitive-services/bing-web-search/\n",
        "    'bing_api_key': os.getenv('BING_API_KEY', ''),\n",
        "    'bing_endpoint': os.getenv('BING_ENDPOINT', 'https://api.bing.microsoft.com/v7.0/search'),\n",
        "\n",
        "    # SerpAPI (Bing engine)\n",
        "    # Docs: https://serpapi.com/\n",
        "    'serpapi_key': os.getenv('SERPAPI_KEY', ''),\n",
        "\n",
        "    # SearXNG public/self-hosted endpoint\n",
        "    # Example public: https://search.bus-hit.me\n",
        "    'searxng_base': os.getenv('SEARXNG_BASE', 'https://search.bus-hit.me'),\n",
        "\n",
        "    # HTML providers config\n",
        "    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'timeout': 15,\n",
        "}\n",
        "\n",
        "# Basic validation helper\n",
        "\n",
        "def require(value: str, name: str) -> None:\n",
        "    if not value:\n",
        "        raise ValueError(f\"Missing required config: {name}. Set env var or edit CONFIG.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connectivity diagnostics (optional)\n",
        "import socket\n",
        "\n",
        "print('— Connectivity checks —')\n",
        "try:\n",
        "    r = requests.get('https://example.com', timeout=10)\n",
        "    print('example.com:', r.status_code)\n",
        "except Exception as e:\n",
        "    print('example.com failed:', type(e).__name__, str(e)[:200])\n",
        "\n",
        "try:\n",
        "    ip = requests.get('https://api.ipify.org?format=json', timeout=10)\n",
        "    print('Public IP:', ip.json().get('ip'))\n",
        "except Exception as e:\n",
        "    print('Public IP check failed:', type(e).__name__, str(e)[:200])\n",
        "\n",
        "print('HTTP_PROXY:', os.getenv('HTTP_PROXY') or os.getenv('http_proxy'))\n",
        "print('HTTPS_PROXY:', os.getenv('HTTPS_PROXY') or os.getenv('https_proxy'))\n",
        "try:\n",
        "    print('Hostname:', socket.gethostname())\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provider implementations\n",
        "\n",
        "headers_common = {\n",
        "    'User-Agent': CONFIG['user_agent'],\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "}\n",
        "\n",
        "\n",
        "def search_bing_api(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    require(CONFIG['bing_api_key'], 'bing_api_key')\n",
        "    url = CONFIG['bing_endpoint']\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'count': max_results,\n",
        "        'mkt': 'en-US',\n",
        "        'responseFilter': 'Webpages'\n",
        "    }\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': CONFIG['bing_api_key']\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, headers=headers, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for w in (data.get('webPages', {}) or {}).get('value', []):\n",
        "        items.append({\n",
        "            'title': w.get('name', ''),\n",
        "            'snippet': truncate(w.get('snippet', ''), 200),\n",
        "            'url': w.get('url', ''),\n",
        "            'source': 'Bing API'\n",
        "        })\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_serpapi_bing(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    require(CONFIG['serpapi_key'], 'serpapi_key')\n",
        "    url = 'https://serpapi.com/search.json'\n",
        "    params = {\n",
        "        'engine': 'bing',\n",
        "        'q': query,\n",
        "        'num': max_results,\n",
        "        'api_key': CONFIG['serpapi_key']\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for res in data.get('organic_results', [])[:max_results]:\n",
        "        items.append({\n",
        "            'title': res.get('title', ''),\n",
        "            'snippet': truncate(res.get('snippet', ''), 200),\n",
        "            'url': res.get('link', ''),\n",
        "            'source': 'SerpAPI (Bing)'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    base = CONFIG['searxng_base'].rstrip('/')\n",
        "    url = f\"{base}/search\"\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'format': 'json',\n",
        "        'engines': 'bing,duckduckgo',\n",
        "        'language': 'en'\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for res in data.get('results', [])[:max_results]:\n",
        "        items.append({\n",
        "            'title': res.get('title', ''),\n",
        "            'snippet': truncate(res.get('content', ''), 200),\n",
        "            'url': res.get('url', ''),\n",
        "            'source': 'SearXNG'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    url = 'https://html.duckduckgo.com/html/'\n",
        "    payload = {'q': query}\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    with Timer() as t:\n",
        "        r = session.post(url, data=payload, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    for element in soup.find_all('div', class_='result')[:max_results]:\n",
        "        a = element.find('a', class_='result__a')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        snippet_elem = element.find('a', class_='result__snippet')\n",
        "        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
        "        if not title or not href:\n",
        "            continue\n",
        "        items.append({\n",
        "            'title': title,\n",
        "            'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
        "            'url': href,\n",
        "            'source': 'DuckDuckGo (HTML)'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    from urllib.parse import quote_plus\n",
        "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    with Timer() as t:\n",
        "        r = session.get(url, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    # Brave changes often; attempt multiple selectors\n",
        "    # Primary: div.snippet with a.result-header\n",
        "    for element in soup.find_all('div', class_='snippet'):\n",
        "        a = element.find('a', class_='result-header')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        desc = ''\n",
        "        p = element.find('p', class_='snippet-description')\n",
        "        if p:\n",
        "            desc = p.get_text(strip=True)\n",
        "        if title and href and href.startswith('http'):\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
        "                'url': href,\n",
        "                'source': 'Brave (HTML)'\n",
        "            })\n",
        "        if len(items) >= max_results:\n",
        "            break\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark harness\n",
        "\n",
        "PROVIDERS = [\n",
        "    ('Bing API', search_bing_api),\n",
        "    ('SerpAPI (Bing)', search_serpapi_bing),\n",
        "    ('SearXNG', search_searxng),\n",
        "    ('DuckDuckGo HTML', search_duckduckgo_html),\n",
        "    ('Brave HTML', search_brave_html),\n",
        "]\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    'python web scraping tutorial',\n",
        "    'latest LLM research 2025',\n",
        "    'how to use pandas merge',\n",
        "]\n",
        "\n",
        "MAX_RESULTS = 5\n",
        "\n",
        "\n",
        "def run_single(provider_name: str, fn, query: str, max_results: int = MAX_RESULTS) -> Dict[str, Any]:\n",
        "    try:\n",
        "        results, meta = fn(query, max_results=max_results)\n",
        "        return {\n",
        "            'provider': provider_name,\n",
        "            'query': query,\n",
        "            'ok': True,\n",
        "            'count': len(results),\n",
        "            'latency_ms': meta.get('latency_ms', None),\n",
        "            'status': meta.get('status', None),\n",
        "            'sample': results[:2] if results else []\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'provider': provider_name,\n",
        "            'query': query,\n",
        "            'ok': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "def run_benchmarks(queries: List[str] = TEST_QUERIES) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for q in queries:\n",
        "        for name, fn in PROVIDERS:\n",
        "            rows.append(run_single(name, fn, q))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SearXNG: SSL fallback, proxy support, and endpoint rotation\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SEARXNG_ENDPOINTS = [\n",
        "    CONFIG['searxng_base'],\n",
        "    'https://searx.be',\n",
        "    'https://searx.tiekoetter.com',\n",
        "    'https://searxng.site',\n",
        "]\n",
        "\n",
        "PROXY_ENV = {\n",
        "    'http': os.getenv('HTTP_PROXY') or os.getenv('http_proxy') or None,\n",
        "    'https': os.getenv('HTTPS_PROXY') or os.getenv('https_proxy') or None,\n",
        "}\n",
        "\n",
        "# Replace search_searxng with a hardened version\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    headers = dict(headers_common)\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    # Try multiple endpoints, handle SSL errors, and optionally disable verification as last resort\n",
        "    last_exc = None\n",
        "    for idx, base in enumerate([e for e in SEARXNG_ENDPOINTS if e]):\n",
        "        base = base.rstrip('/')\n",
        "        url = f\"{base}/search\"\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'format': 'json',\n",
        "            'engines': 'bing,duckduckgo',\n",
        "            'language': 'en'\n",
        "        }\n",
        "\n",
        "        # Attempt 1: normal GET with proxies if set\n",
        "        try:\n",
        "            with Timer() as t:\n",
        "                r = session.get(url, params=params, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "                r.raise_for_status()\n",
        "                data = r.json()\n",
        "            items = []\n",
        "            for res in data.get('results', [])[:max_results]:\n",
        "                items.append({\n",
        "                    'title': res.get('title', ''),\n",
        "                    'snippet': truncate(res.get('content', ''), 200),\n",
        "                    'url': res.get('url', ''),\n",
        "                    'source': f\"SearXNG ({urlparse(base).netloc})\"\n",
        "                })\n",
        "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "        except requests.exceptions.SSLError as e:\n",
        "            last_exc = e\n",
        "            # Attempt 2: allow self-signed/invalid certs (verification off) for lab environments\n",
        "            try:\n",
        "                with Timer() as t:\n",
        "                    r = session.get(url, params=params, timeout=CONFIG['timeout'], verify=False, proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "                    r.raise_for_status()\n",
        "                    data = r.json()\n",
        "                items = []\n",
        "                for res in data.get('results', [])[:max_results]:\n",
        "                    items.append({\n",
        "                        'title': res.get('title', ''),\n",
        "                        'snippet': truncate(res.get('content', ''), 200),\n",
        "                        'url': res.get('url', ''),\n",
        "                        'source': f\"SearXNG (insecure) ({urlparse(base).netloc})\"\n",
        "                    })\n",
        "                return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "            except Exception as e2:\n",
        "                last_exc = e2\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            continue\n",
        "\n",
        "    raise RuntimeError(f\"SearXNG failed across endpoints: {type(last_exc).__name__}: {str(last_exc)[:200]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run and report\n",
        "\n",
        "# Optional: quickly check which providers are enabled by config\n",
        "print('Config summary:')\n",
        "print('- bing_api_key set:', bool(CONFIG['bing_api_key']))\n",
        "print('- serpapi_key set:', bool(CONFIG['serpapi_key']))\n",
        "print('- searxng_base:', CONFIG['searxng_base'])\n",
        "\n",
        "# Execute benchmarks\n",
        "results_df = run_benchmarks(TEST_QUERIES)\n",
        "\n",
        "# Summary table\n",
        "summary_cols = ['provider', 'ok', 'count', 'latency_ms', 'status', 'query', 'error']\n",
        "summary = results_df.reindex(columns=[c for c in summary_cols if c in results_df.columns])\n",
        "print('\\nSummary:')\n",
        "print(tabulate(summary.fillna(''), headers='keys', tablefmt='github'))\n",
        "\n",
        "# Show sample results for successful runs\n",
        "print('\\nSamples:')\n",
        "for _, row in results_df.iterrows():\n",
        "    if row.get('ok') and row.get('sample'):\n",
        "        print(f\"\\n== {row['provider']} | Query: {row['query']} | Count: {row['count']} | {row.get('latency_ms', '')} ms ==\")\n",
        "        for i, item in enumerate(row['sample'], 1):\n",
        "            print(f\"{i}. {item.get('title','')}\")\n",
        "            if item.get('snippet'):\n",
        "                print(f\"   {truncate(item['snippet'], 160)}\")\n",
        "            if item.get('url'):\n",
        "                print(f\"   URL: {item['url']}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
