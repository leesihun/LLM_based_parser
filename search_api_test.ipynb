{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search API Test Notebook\n",
    "\n",
    "This self-contained notebook tests multiple web search providers and reports latency, status, and top results.\n",
    "\n",
    "Providers included:\n",
    "- Bing Web Search API (Azure Cognitive Services)\n",
    "- SerpAPI (engine=bing)\n",
    "- SearXNG (public or self-hosted)\n",
    "- DuckDuckGo HTML (no-JS HTML endpoint)\n",
    "- Brave HTML\n",
    "\n",
    "Instructions:\n",
    "- Set API keys/endpoints in the Config cell below (or via environment variables).\n",
    "- Run the cells top-to-bottom. The notebook installs missing dependencies at runtime.\n",
    "- Results include a summary table and sample results per provider.\n",
    "\n",
    "Notes:\n",
    "- Use responsibly and comply with each provider's Terms of Service.\n",
    "- HTML providers may change markup; simple parsers here are best-effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime setup: imports and on-the-fly installs\n",
    "import sys, subprocess, json, os, time, random\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Lazy installer\n",
    "\n",
    "def ensure(package: str, import_name: Optional[str] = None):\n",
    "    try:\n",
    "        __import__(import_name or package)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "# Ensure dependencies used by HTML parsers and table rendering\n",
    "for pkg, imp in [\n",
    "    ('requests', 'requests'),\n",
    "    ('beautifulsoup4', 'bs4'),\n",
    "    ('pandas', 'pandas'),\n",
    "    ('tabulate', 'tabulate'),\n",
    "]:\n",
    "    ensure(pkg, imp)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Utility: timing\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.end = time.time()\n",
    "        self.ms = int((self.end - self.start) * 1000)\n",
    "\n",
    "# Normalize result schema\n",
    "Result = Dict[str, str]\n",
    "\n",
    "\n",
    "def truncate(text: str, n: int = 200) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    return text if len(text) <= n else text[:n] + '...'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: API keys and endpoints\n",
    "CONFIG = {\n",
    "    # Bing Web Search API (Azure)\n",
    "    # Docs: https://learn.microsoft.com/azure/cognitive-services/bing-web-search/\n",
    "    'bing_api_key': os.getenv('BING_API_KEY', ''),\n",
    "    'bing_endpoint': os.getenv('BING_ENDPOINT', 'https://api.bing.microsoft.com/v7.0/search'),\n",
    "\n",
    "    # SerpAPI (Bing engine)\n",
    "    # Docs: https://serpapi.com/\n",
    "    'serpapi_key': os.getenv('SERPAPI_KEY', ''),\n",
    "\n",
    "    # SearXNG public/self-hosted endpoint\n",
    "    # Example remote fallback: https://search.bus-hit.me\n",
    "    'searxng_base': os.getenv('SEARXNG_BASE', 'http://192.168.219.113:8080'),\n",
    "    # Optional internal-only base (WSL bridge)\n",
    "    'searxng_internal_base': os.getenv('SEARXNG_INTERNAL_BASE', 'http://localhost:8080'),\n",
    "\n",
    "    # HTML providers config\n",
    "    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'timeout': 15,\n",
    "}\n",
    "\n",
    "# Basic validation helper\n",
    "\n",
    "def require(value: str, name: str) -> None:\n",
    "    if not value:\n",
    "        raise ValueError(f\"Missing required config: {name}. Set env var or edit CONFIG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??Connectivity checks ??\n",
      "example.com: 200\n",
      "Public IP: 115.138.25.219\n",
      "HTTP_PROXY: None\n",
      "HTTPS_PROXY: None\n",
      "Hostname: DESKTOP-TBVDNCR\n"
     ]
    }
   ],
   "source": [
    "# Connectivity diagnostics (optional)\n",
    "import socket\n",
    "\n",
    "print('??Connectivity checks ??')\n",
    "try:\n",
    "    r = requests.get('https://example.com', timeout=10)\n",
    "    print('example.com:', r.status_code)\n",
    "except Exception as e:\n",
    "    print('example.com failed:', type(e).__name__, str(e)[:200])\n",
    "\n",
    "try:\n",
    "    ip = requests.get('https://api.ipify.org?format=json', timeout=10)\n",
    "    print('Public IP:', ip.json().get('ip'))\n",
    "except Exception as e:\n",
    "    print('Public IP check failed:', type(e).__name__, str(e)[:200])\n",
    "\n",
    "print('HTTP_PROXY:', os.getenv('HTTP_PROXY') or os.getenv('http_proxy'))\n",
    "print('HTTPS_PROXY:', os.getenv('HTTPS_PROXY') or os.getenv('https_proxy'))\n",
    "try:\n",
    "    print('Hostname:', socket.gethostname())\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider implementations\n",
    "\n",
    "headers_common = {\n",
    "    'User-Agent': CONFIG['user_agent'],\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "}\n",
    "\n",
    "\n",
    "def search_bing_api(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    require(CONFIG['bing_api_key'], 'bing_api_key')\n",
    "    url = CONFIG['bing_endpoint']\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'count': max_results,\n",
    "        'mkt': 'en-US',\n",
    "        'responseFilter': 'Webpages'\n",
    "    }\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': CONFIG['bing_api_key']\n",
    "    }\n",
    "    with Timer() as t:\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=CONFIG['timeout'])\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    items = []\n",
    "    for w in (data.get('webPages', {}) or {}).get('value', []):\n",
    "        items.append({\n",
    "            'title': w.get('name', ''),\n",
    "            'snippet': truncate(w.get('snippet', ''), 200),\n",
    "            'url': w.get('url', ''),\n",
    "            'source': 'Bing API'\n",
    "        })\n",
    "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_serpapi_bing(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    require(CONFIG['serpapi_key'], 'serpapi_key')\n",
    "    url = 'https://serpapi.com/search.json'\n",
    "    params = {\n",
    "        'engine': 'bing',\n",
    "        'q': query,\n",
    "        'num': max_results,\n",
    "        'api_key': CONFIG['serpapi_key']\n",
    "    }\n",
    "    with Timer() as t:\n",
    "        r = requests.get(url, params=params, timeout=CONFIG['timeout'])\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    items = []\n",
    "    for res in data.get('organic_results', [])[:max_results]:\n",
    "        items.append({\n",
    "            'title': res.get('title', ''),\n",
    "            'snippet': truncate(res.get('snippet', ''), 200),\n",
    "            'url': res.get('link', ''),\n",
    "            'source': 'SerpAPI (Bing)'\n",
    "        })\n",
    "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    base = CONFIG.get('searxng_base', 'http://172.20.221.97:8080').rstrip('/')\n",
    "    url = f\"{base}/search\"\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'format': 'json',\n",
    "        'engines': 'bing,duckduckgo',\n",
    "        'language': 'en'\n",
    "    }\n",
    "    with Timer() as t:\n",
    "        r = requests.get(url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    items = []\n",
    "    for res in data.get('results', [])[:max_results]:\n",
    "        items.append({\n",
    "            'title': res.get('title', ''),\n",
    "            'snippet': truncate(res.get('content', ''), 200),\n",
    "            'url': res.get('url', ''),\n",
    "            'source': 'SearXNG'\n",
    "        })\n",
    "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    url = 'https://html.duckduckgo.com/html/'\n",
    "    payload = {'q': query}\n",
    "    session = requests.Session()\n",
    "    session.trust_env = False\n",
    "    session.headers.update(headers_common)\n",
    "    with Timer() as t:\n",
    "        r = session.post(url, data=payload, timeout=CONFIG['timeout'])\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    items: List[Result] = []\n",
    "    for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "        a = element.find('a', class_='result__a')\n",
    "        if not a:\n",
    "            continue\n",
    "        title = a.get_text(strip=True) or ''\n",
    "        href = a.get('href', '')\n",
    "        snippet_elem = element.find('a', class_='result__snippet')\n",
    "        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "        if not title or not href:\n",
    "            continue\n",
    "        items.append({\n",
    "            'title': title,\n",
    "            'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
    "            'url': href,\n",
    "            'source': 'DuckDuckGo (HTML)'\n",
    "        })\n",
    "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    from urllib.parse import quote_plus\n",
    "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
    "    session.trust_env = False\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers_common)\n",
    "    with Timer() as t:\n",
    "        r = session.get(url, timeout=CONFIG['timeout'])\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    items: List[Result] = []\n",
    "    # Brave changes often; attempt multiple selectors\n",
    "    # Primary: div.snippet with a.result-header\n",
    "    for element in soup.find_all('div', class_='snippet'):\n",
    "        a = element.find('a', class_='result-header')\n",
    "        if not a:\n",
    "            continue\n",
    "        title = a.get_text(strip=True) or ''\n",
    "        href = a.get('href', '')\n",
    "        desc = ''\n",
    "        p = element.find('p', class_='snippet-description')\n",
    "        if p:\n",
    "            desc = p.get_text(strip=True)\n",
    "        if title and href and href.startswith('http'):\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
    "                'url': href,\n",
    "                'source': 'Brave (HTML)'\n",
    "            })\n",
    "        if len(items) >= max_results:\n",
    "            break\n",
    "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark harness\n",
    "\n",
    "PROVIDERS = [\n",
    "    ('Bing API', search_bing_api),\n",
    "    ('SerpAPI (Bing)', search_serpapi_bing),\n",
    "    ('SearXNG', search_searxng),\n",
    "    ('DuckDuckGo HTML', search_duckduckgo_html),\n",
    "    ('Brave HTML', search_brave_html),\n",
    "]\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    'python web scraping tutorial',\n",
    "    'latest LLM research 2025',\n",
    "    'how to use pandas merge',\n",
    "]\n",
    "\n",
    "MAX_RESULTS = 5\n",
    "\n",
    "\n",
    "def run_single(provider_name: str, fn, query: str, max_results: int = MAX_RESULTS) -> Dict[str, Any]:\n",
    "    try:\n",
    "        results, meta = fn(query, max_results=max_results)\n",
    "        return {\n",
    "            'provider': provider_name,\n",
    "            'query': query,\n",
    "            'ok': True,\n",
    "            'count': len(results),\n",
    "            'latency_ms': meta.get('latency_ms', None),\n",
    "            'status': meta.get('status', None),\n",
    "            'sample': results[:2] if results else []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'provider': provider_name,\n",
    "            'query': query,\n",
    "            'ok': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def run_benchmarks(queries: List[str] = TEST_QUERIES) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q in queries:\n",
    "        for name, fn in PROVIDERS:\n",
    "            rows.append(run_single(name, fn, q))\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SearXNG: SSL fallback, proxy support, and endpoint rotation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "SEARXNG_ENDPOINTS = [\n",
    "    CONFIG.get('searxng_base', 'http://172.20.221.97:8080'),\n",
    "    # Optional internal-only base (WSL bridge)\n",
    "    CONFIG.get('searxng_internal_base', 'http://localhost:8080'),\n",
    "    'https://searx.be',\n",
    "    'https://search.bus-hit.me',\n",
    "]\n",
    "\n",
    "PROXY_ENV = {\n",
    "    'http': os.getenv('HTTP_PROXY') or os.getenv('http_proxy') or None,\n",
    "    'https': os.getenv('HTTPS_PROXY') or os.getenv('https_proxy') or None,\n",
    "}\n",
    "\n",
    "# Replace search_searxng with a hardened version\n",
    "\n",
    "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    headers = dict(headers_common)\n",
    "    session = requests.Session()\n",
    "    session.trust_env = False\n",
    "    session.headers.update(headers)\n",
    "\n",
    "    # Try multiple endpoints, handle SSL errors, and optionally disable verification as last resort\n",
    "    last_exc = None\n",
    "    for idx, base in enumerate([e for e in SEARXNG_ENDPOINTS if e]):\n",
    "        base = base.rstrip('/')\n",
    "        url = f\"{base}/search\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'format': 'json',\n",
    "            'engines': 'bing,duckduckgo',\n",
    "            'language': 'en'\n",
    "        }\n",
    "\n",
    "        # Attempt 1: normal GET with proxies if set\n",
    "        try:\n",
    "            with Timer() as t:\n",
    "                r = session.get(url, params=params, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "            items = []\n",
    "            for res in data.get('results', [])[:max_results]:\n",
    "                items.append({\n",
    "                    'title': res.get('title', ''),\n",
    "                    'snippet': truncate(res.get('content', ''), 200),\n",
    "                    'url': res.get('url', ''),\n",
    "                    'source': f\"SearXNG ({urlparse(base).netloc})\"\n",
    "                })\n",
    "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            last_exc = e\n",
    "            # Attempt 2: allow self-signed/invalid certs (verification off) for lab environments\n",
    "            try:\n",
    "                with Timer() as t:\n",
    "                    r = session.get(url, params=params, timeout=CONFIG['timeout'], verify=False, proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
    "                    r.raise_for_status()\n",
    "                    data = r.json()\n",
    "                items = []\n",
    "                for res in data.get('results', [])[:max_results]:\n",
    "                    items.append({\n",
    "                        'title': res.get('title', ''),\n",
    "                        'snippet': truncate(res.get('content', ''), 200),\n",
    "                        'url': res.get('url', ''),\n",
    "                        'source': f\"SearXNG (insecure) ({urlparse(base).netloc})\"\n",
    "                    })\n",
    "                return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "            except Exception as e2:\n",
    "                last_exc = e2\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"SearXNG failed across endpoints: {type(last_exc).__name__}: {str(last_exc)[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML providers: SSL fallback and proxy support\n",
    "\n",
    "PROXY_ENV = {\n",
    "    'http': os.getenv('HTTP_PROXY') or os.getenv('http_proxy') or None,\n",
    "    'https': os.getenv('HTTPS_PROXY') or os.getenv('https_proxy') or None,\n",
    "}\n",
    "\n",
    "ALLOW_INSECURE_SSL = bool(os.getenv('ALLOW_INSECURE_SSL', '0') in ['1', 'true', 'True'])\n",
    "\n",
    "# Wrap the existing HTML functions with hardened versions\n",
    "\n",
    "_original_ddg = search_duckduckgo_html\n",
    "_original_brave = search_brave_html\n",
    "\n",
    "\n",
    "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    url = 'https://html.duckduckgo.com/html/'\n",
    "    payload = {'q': query}\n",
    "    session = requests.Session()\n",
    "    session.trust_env = False\n",
    "    session.headers.update(headers_common)\n",
    "    # Attempt 1: normal POST\n",
    "    try:\n",
    "        with Timer() as t:\n",
    "            r = session.post(url, data=payload, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        items: List[Result] = []\n",
    "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "            a = element.find('a', class_='result__a')\n",
    "            if not a:\n",
    "                continue\n",
    "            title = a.get_text(strip=True) or ''\n",
    "            href = a.get('href', '')\n",
    "            snippet_elem = element.find('a', class_='result__snippet')\n",
    "            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
    "                'url': href,\n",
    "                'source': 'DuckDuckGo (HTML)'\n",
    "            })\n",
    "        return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        if not ALLOW_INSECURE_SSL:\n",
    "            raise\n",
    "        # Attempt 2: insecure SSL for lab envs\n",
    "        with Timer() as t:\n",
    "            r = session.post(url, data=payload, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None, verify=False)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        items: List[Result] = []\n",
    "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "            a = element.find('a', class_='result__a')\n",
    "            if not a:\n",
    "                continue\n",
    "            title = a.get_text(strip=True) or ''\n",
    "            href = a.get('href', '')\n",
    "            snippet_elem = element.find('a', class_='result__snippet')\n",
    "            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
    "                'url': href,\n",
    "                'source': 'DuckDuckGo (HTML, insecure)'\n",
    "            })\n",
    "        return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    from urllib.parse import quote_plus\n",
    "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
    "    session.trust_env = False\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers_common)\n",
    "    # Attempt 1: normal GET\n",
    "    try:\n",
    "        with Timer() as t:\n",
    "            r = session.get(url, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        if not ALLOW_INSECURE_SSL:\n",
    "            raise\n",
    "        with Timer() as t:\n",
    "            r = session.get(url, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None, verify=False)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    items: List[Result] = []\n",
    "    for element in soup.find_all('div', class_='snippet'):\n",
    "        a = element.find('a', class_='result-header')\n",
    "        if not a:\n",
    "            continue\n",
    "        title = a.get_text(strip=True) or ''\n",
    "        href = a.get('href', '')\n",
    "        desc = ''\n",
    "        p = element.find('p', class_='snippet-description')\n",
    "        if p:\n",
    "            desc = p.get_text(strip=True)\n",
    "        if title and href and href.startswith('http'):\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
    "                'url': href,\n",
    "                'source': 'Brave (HTML, insecure)' if 'verify=False' in str(r.request.__dict__) else 'Brave (HTML)'\n",
    "            })\n",
    "        if len(items) >= max_results:\n",
    "            break\n",
    "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??Network wrapper ready ??\n",
      "PROXIES: {}\n",
      "CA_BUNDLE_PATH: (default trust store)\n",
      "ALLOW_INSECURE_SSL: False\n",
      "RETRIES: 2 BACKOFF: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Unified request helper: proxies, CA bundle, retries, insecure fallback\n",
    "import urllib3\n",
    "from requests.exceptions import SSLError, ProxyError, ConnectionError, ReadTimeout\n",
    "\n",
    "# Optional config via env or direct edits here\n",
    "PROXY_URL = os.getenv('PROXY_URL', '').strip()  # e.g., http://user:pass@host:port\n",
    "PROXY_USERNAME = os.getenv('PROXY_USERNAME', '').strip()\n",
    "PROXY_PASSWORD = os.getenv('PROXY_PASSWORD', '').strip()\n",
    "CA_BUNDLE_PATH = os.getenv('CA_BUNDLE_PATH', '').strip()  # e.g., C:\\certs\\corp_ca.pem\n",
    "RETRIES = int(os.getenv('REQUEST_RETRIES', '2'))\n",
    "BACKOFF = float(os.getenv('REQUEST_BACKOFF', '0.7'))\n",
    "\n",
    "if ALLOW_INSECURE_SSL:\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "def build_proxies() -> Optional[Dict[str, str]]:\n",
    "    # Prefer explicit PROXY_URL if provided; else environment proxies\n",
    "    if PROXY_URL:\n",
    "        return {'http': PROXY_URL, 'https': PROXY_URL}\n",
    "    if any(PROXY_ENV.values()):\n",
    "        return PROXY_ENV\n",
    "    # Support basic auth if host provided separately (rare); construct URL\n",
    "    if os.getenv('PROXY_HOST') and os.getenv('PROXY_PORT'):\n",
    "        host = os.getenv('PROXY_HOST').strip()\n",
    "        port = os.getenv('PROXY_PORT').strip()\n",
    "        user = PROXY_USERNAME or os.getenv('PROXY_USERNAME', '').strip()\n",
    "        pwd = PROXY_PASSWORD or os.getenv('PROXY_PASSWORD', '').strip()\n",
    "        if user and pwd:\n",
    "            return {\n",
    "                'http': f'http://{user}:{pwd}@{host}:{port}',\n",
    "                'https': f'http://{user}:{pwd}@{host}:{port}'\n",
    "            }\n",
    "        return {'http': f'http://{host}:{port}', 'https': f'http://{host}:{port}'}\n",
    "    return {}  # disable system proxy usage when not configured\n",
    "\n",
    "\n",
    "PROXIES = build_proxies()\n",
    "\n",
    "\n",
    "def request_with_fallback(method: str, url: str, *, params=None, data=None, headers=None, timeout=None) -> requests.Response:\n",
    "    last_exc = None\n",
    "    attempts = []\n",
    "\n",
    "    # Strategy order: explicit CA bundle -> default verify -> insecure if allowed\n",
    "    strategies = []\n",
    "    if CA_BUNDLE_PATH:\n",
    "        strategies.append(('verify_path', CA_BUNDLE_PATH))\n",
    "    strategies.append(('verify_true', True))\n",
    "    if ALLOW_INSECURE_SSL:\n",
    "        strategies.append(('verify_false', False))\n",
    "\n",
    "    for attempt in range(RETRIES + 1):\n",
    "        for label, verify in strategies:\n",
    "            try:\n",
    "                resp = requests.request(\n",
    "                    method,\n",
    "                    url,\n",
    "                    params=params,\n",
    "                    data=data,\n",
    "                    headers=headers,\n",
    "                    timeout=timeout or CONFIG['timeout'],\n",
    "                    proxies=PROXIES,\n",
    "                    verify=verify\n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "                return resp\n",
    "            except (SSLError, ProxyError, ConnectionError, ReadTimeout) as e:\n",
    "                last_exc = e\n",
    "                attempts.append(f\"{label}:{type(e).__name__}\")\n",
    "                time.sleep(BACKOFF * (attempt + 1))\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                attempts.append(f\"{label}:{type(e).__name__}\")\n",
    "                time.sleep(BACKOFF * (attempt + 1))\n",
    "    raise RuntimeError(f\"Request failed after retries: {type(last_exc).__name__}: {str(last_exc)[:200]} | attempts={attempts}\")\n",
    "\n",
    "\n",
    "print('??Network wrapper ready ??')\n",
    "print('PROXIES:', PROXIES)\n",
    "print('CA_BUNDLE_PATH:', CA_BUNDLE_PATH or '(default trust store)')\n",
    "print('ALLOW_INSECURE_SSL:', ALLOW_INSECURE_SSL)\n",
    "print('RETRIES:', RETRIES, 'BACKOFF:', BACKOFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers rebound to robust network wrapper\n"
     ]
    }
   ],
   "source": [
    "# Rebind providers to use the unified request helper\n",
    "\n",
    "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    from urllib.parse import urlparse\n",
    "    last_exc = None\n",
    "    for base in [e for e in SEARXNG_ENDPOINTS if e]:\n",
    "        base = base.rstrip('/')\n",
    "        url = f\"{base}/search\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'format': 'json',\n",
    "            'engines': 'bing,duckduckgo',\n",
    "            'language': 'en'\n",
    "        }\n",
    "        try:\n",
    "            with Timer() as t:\n",
    "                r = request_with_fallback('GET', url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
    "                data = r.json()\n",
    "            items = []\n",
    "            for res in data.get('results', [])[:max_results]:\n",
    "                items.append({\n",
    "                    'title': res.get('title', ''),\n",
    "                    'snippet': truncate(res.get('content', ''), 200),\n",
    "                    'url': res.get('url', ''),\n",
    "                    'source': f\"SearXNG ({urlparse(base).netloc})\"\n",
    "                })\n",
    "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"SearXNG failed: {type(last_exc).__name__}: {str(last_exc)[:200]}\")\n",
    "\n",
    "\n",
    "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    url = 'https://html.duckduckgo.com/html/'\n",
    "    payload = {'q': query}\n",
    "    with Timer() as t:\n",
    "        r = request_with_fallback('POST', url, data=payload, headers=headers_common, timeout=CONFIG['timeout'])\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    items: List[Result] = []\n",
    "    for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "        a = element.find('a', class_='result__a')\n",
    "        if not a:\n",
    "            continue\n",
    "        title = a.get_text(strip=True) or ''\n",
    "        href = a.get('href', '')\n",
    "        snippet_elem = element.find('a', class_='result__snippet')\n",
    "        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "        if not title or not href:\n",
    "            continue\n",
    "        items.append({\n",
    "            'title': title,\n",
    "            'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
    "            'url': href,\n",
    "            'source': 'DuckDuckGo (HTML)'\n",
    "        })\n",
    "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "\n",
    "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    from urllib.parse import quote_plus\n",
    "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
    "    with Timer() as t:\n",
    "        r = request_with_fallback('GET', url, headers=headers_common, timeout=CONFIG['timeout'])\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    items: List[Result] = []\n",
    "    for element in soup.find_all('div', class_='snippet'):\n",
    "        a = element.find('a', class_='result-header')\n",
    "        if not a:\n",
    "            continue\n",
    "        title = a.get_text(strip=True) or ''\n",
    "        href = a.get('href', '')\n",
    "        desc = ''\n",
    "        p = element.find('p', class_='snippet-description')\n",
    "        if p:\n",
    "            desc = p.get_text(strip=True)\n",
    "        if title and href and href.startswith('http'):\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
    "                'url': href,\n",
    "                'source': 'Brave (HTML)'\n",
    "            })\n",
    "        if len(items) >= max_results:\n",
    "            break\n",
    "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
    "\n",
    "# Update provider list to use the rebound functions\n",
    "PROVIDERS = [\n",
    "    ('Bing API', search_bing_api),\n",
    "    ('SerpAPI (Bing)', search_serpapi_bing),\n",
    "    ('SearXNG', search_searxng),\n",
    "    ('DuckDuckGo HTML', search_duckduckgo_html),\n",
    "    ('Brave HTML', search_brave_html),\n",
    "]\n",
    "\n",
    "print('Providers rebound to robust network wrapper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Proxy/SSL Diagnostics 🔍\n",
      "HTTP_PROXY: None\n",
      "HTTPS_PROXY: None\n",
      "PROXY_URL: None\n",
      "PROXY_HOST: None\n",
      "PROXY_PORT: None\n",
      "CA_BUNDLE_PATH: None\n",
      "ALLOW_INSECURE_SSL: None\n",
      "OK https://example.com in 761 ms - status 200\n",
      "FAIL http://192.168.219.113:8080/search: RuntimeError Request failed after retries: ReadTimeout: HTTPConnectionPool(host='192.168.219.113', port=8080): Read timed out. (read timeout=10) | attempts=['verify_true:ReadTimeout', 'verify_true:ReadTimeout', 'verify_true:ReadTimeout']\n",
      "certifi.where(): C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages\\certifi\\cacert.pem\n",
      "SSL context created. Custom CA used: False\n"
     ]
    }
   ],
   "source": [
    "# Proxy/SSL diagnostics (company network)\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "print('🔍 Proxy/SSL Diagnostics 🔍')\n",
    "print('HTTP_PROXY:', os.getenv('HTTP_PROXY') or os.getenv('http_proxy'))\n",
    "print('HTTPS_PROXY:', os.getenv('HTTPS_PROXY') or os.getenv('https_proxy'))\n",
    "print('PROXY_URL:', os.getenv('PROXY_URL'))\n",
    "print('PROXY_HOST:', os.getenv('PROXY_HOST'))\n",
    "print('PROXY_PORT:', os.getenv('PROXY_PORT'))\n",
    "print('CA_BUNDLE_PATH:', os.getenv('CA_BUNDLE_PATH'))\n",
    "print('ALLOW_INSECURE_SSL:', os.getenv('ALLOW_INSECURE_SSL'))\n",
    "\n",
    "# Test DNS and TCP via proxies if configured\n",
    "TEST_URLS = [\n",
    "    'https://example.com',\n",
    "    CONFIG.get('searxng_base', 'http://172.20.221.97:8080').rstrip('/') + '/search'\n",
    "]\n",
    "\n",
    "for u in TEST_URLS:\n",
    "    try:\n",
    "        with Timer() as t:\n",
    "            r = request_with_fallback('GET', u, headers={'User-Agent': CONFIG['user_agent']}, timeout=10)\n",
    "        print(f'OK {u} in {t.ms} ms - status {r.status_code}')\n",
    "    except Exception as e:\n",
    "        print(f'FAIL {u}:', type(e).__name__, str(e)[:300])\n",
    "\n",
    "# Show default cert store in use\n",
    "try:\n",
    "    print('certifi.where():', certifi.where())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    ctx = ssl.create_default_context(cafile=os.getenv('CA_BUNDLE_PATH') or None)\n",
    "    print('SSL context created. Custom CA used:', bool(os.getenv('CA_BUNDLE_PATH')))\n",
    "except Exception as e:\n",
    "    print('SSL context error:', type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing SearXNG directly via URL\n",
    "\n",
    "You can open a SearXNG instance in your browser and run a search by navigating to:\n",
    "\n",
    "- Base URL (homepage):\n",
    "  - `https://search.bus-hit.me/`\n",
    "- JSON API endpoint (browser-friendly for quick checks):\n",
    "  - `https://search.bus-hit.me/search?q=python&format=json`\n",
    "- HTML results page:\n",
    "  - `https://search.bus-hit.me/search?q=python`\n",
    "\n",
    "Replace `search.bus-hit.me` with your preferred or company-hosted SearXNG instance.\n",
    "\n",
    "Common public instances (availability varies):\n",
    "- `https://searx.be`\n",
    "- `https://searx.tiekoetter.com`\n",
    "- `https://searxng.site`\n",
    "\n",
    "Tip: If you are behind a corporate proxy with a custom CA, your browser may trust the proxy automatically. If you want Python to trust the same CA, export the CA bundle path and rerun the notebook:\n",
    "- Windows PowerShell:\n",
    "  - `$env:CA_BUNDLE_PATH = \"C:\\\\path\\\\to\\\\corp_ca.pem\"`\n",
    "- CMD:\n",
    "  - `set CA_BUNDLE_PATH=C:\\path\\to\\corp_ca.pem`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearXNG rotation enabled. Ordered candidates: ['http://localhost:8080', 'http://192.168.219.113:8080', 'https://search.bus-hit.me', 'https://searx.be']\n"
     ]
    }
   ],
   "source": [
    "# SearXNG endpoint health and rotation (handle 403 bans)\n",
    "import time as _time\n",
    "from urllib.parse import urlparse as _urlparse\n",
    "from requests.exceptions import HTTPError as _HTTPError\n",
    "\n",
    "SEARXNG_HEALTH = globals().get('SEARXNG_HEALTH', {})  # persist across runs in session\n",
    "SEARXNG_BAN_SECONDS = int(os.getenv('SEARXNG_BAN_SECONDS', '3600'))  # 1 hour default\n",
    "SEARXNG_FAVOR_HOSTS = {'172.20.221.97:8080': 3, 'localhost:8080': 2}  # weight boost for local instance\n",
    "\n",
    "\n",
    "def _now() -> float:\n",
    "    return _time.time()\n",
    "\n",
    "\n",
    "def _host_of(base: str) -> str:\n",
    "    try:\n",
    "        return _urlparse(base).netloc\n",
    "    except Exception:\n",
    "        return base\n",
    "\n",
    "\n",
    "def get_searxng_candidates() -> list:\n",
    "    # Build weighted list with health and bans\n",
    "    candidates = []\n",
    "    now = _now()\n",
    "    for base in [e for e in SEARXNG_ENDPOINTS if e]:\n",
    "        host = _host_of(base)\n",
    "        h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
    "        banned = h.get('banned_until', 0) > now\n",
    "        weight = h.get('ok', 0) - h.get('fail', 0)\n",
    "        weight += SEARXNG_FAVOR_HOSTS.get(host, 0)\n",
    "        candidates.append((banned, -weight, base))\n",
    "    # Not banned first, higher weight first\n",
    "    candidates.sort()\n",
    "    # Return ordered bases\n",
    "    ordered = [base for banned, _w, base in candidates if not banned]\n",
    "    # Append banned ones at the end (last resort if bans expire inside same run)\n",
    "    ordered += [base for banned, _w, base in candidates if banned]\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
    "    ordered = get_searxng_candidates()\n",
    "    last_exc = None\n",
    "    for base in ordered:\n",
    "        base = base.rstrip('/')\n",
    "        host = _host_of(base)\n",
    "        url = f\"{base}/search\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'format': 'json',\n",
    "            'engines': 'bing,duckduckgo',\n",
    "            'language': 'en'\n",
    "        }\n",
    "        try:\n",
    "            with Timer() as t:\n",
    "                r = request_with_fallback('GET', url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
    "                data = r.json()\n",
    "            items = []\n",
    "            for res in data.get('results', [])[:max_results]:\n",
    "                items.append({\n",
    "                    'title': res.get('title', ''),\n",
    "                    'snippet': truncate(res.get('content', ''), 200),\n",
    "                    'url': res.get('url', ''),\n",
    "                    'source': f\"SearXNG ({host})\"\n",
    "                })\n",
    "            # mark success\n",
    "            h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
    "            h['ok'] = h.get('ok', 0) + 1\n",
    "            h['last_ok_at'] = _now()\n",
    "            h['banned_until'] = 0\n",
    "            SEARXNG_HEALTH[host] = h\n",
    "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            emsg = str(e)\n",
    "            # Detect 403 and ban temporarily\n",
    "            if '403' in emsg or 'Forbidden' in emsg:\n",
    "                h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
    "                h['fail'] = h.get('fail', 0) + 1\n",
    "                h['banned_until'] = _now() + SEARXNG_BAN_SECONDS\n",
    "                h['last_status'] = 403\n",
    "                SEARXNG_HEALTH[host] = h\n",
    "            else:\n",
    "                h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
    "                h['fail'] = h.get('fail', 0) + 1\n",
    "                h['last_status'] = getattr(getattr(e, 'response', None), 'status_code', None)\n",
    "                SEARXNG_HEALTH[host] = h\n",
    "            continue\n",
    "    raise RuntimeError(f\"SearXNG all endpoints failed. Last error: {type(last_exc).__name__}: {str(last_exc)[:200]}. Health={SEARXNG_HEALTH}\")\n",
    "\n",
    "print('SearXNG rotation enabled. Ordered candidates:', get_searxng_candidates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certifi-win32 load failed: CalledProcessError Command '['c:\\\\Python313\\\\python.exe', '-m', 'pip', 'install', 'certifi-win32']' returned non-zero exit status 1.\n",
      "SEARXNG_FORCE_BASE not set; using endpoints list as configured\n"
     ]
    }
   ],
   "source": [
    "# Windows corporate CA support (certifi-win32) and optional SearXNG force\n",
    "try:\n",
    "    ensure('certifi-win32', 'certifi_win32')\n",
    "    import certifi_win32  # noqa: F401\n",
    "    print('certifi-win32 loaded: Windows certificate store integrated with certifi')\n",
    "except Exception as e:\n",
    "    print('certifi-win32 load failed:', type(e).__name__, str(e)[:200])\n",
    "\n",
    "# Optionally force a specific SearXNG base via env\n",
    "_force = os.getenv('SEARXNG_FORCE_BASE', '').strip()\n",
    "if _force:\n",
    "    SEARXNG_ENDPOINTS = [_force]\n",
    "    print('SEARXNG_FORCE_BASE set -> using single endpoint:', _force)\n",
    "else:\n",
    "    print('SEARXNG_FORCE_BASE not set; using endpoints list as configured')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config summary:\n",
      "- bing_api_key set: False\n",
      "- serpapi_key set: False\n",
      "- searxng_base: http://192.168.219.113:8080\n",
      "\n",
      "Summary:\n",
      "|    | provider        | ok    | count   | latency_ms   | status   | query                        | error                                                                                                                                                                                                                                                                                                                            |\n",
      "|----|-----------------|-------|---------|--------------|----------|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | Bing API        | False |         |              |          | python web scraping tutorial | Missing required config: bing_api_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                               |\n",
      "|  1 | SerpAPI (Bing)  | False |         |              |          | python web scraping tutorial | Missing required config: serpapi_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                                |\n",
      "|  2 | SearXNG         | False |         |              |          | python web scraping tutorial | SearXNG failed: RuntimeError: Request failed after retries: ConnectionError: HTTPSConnectionPool(host='search.bus-hit.me', port=443): Max retries exceeded with url: /search?q=python+web+scraping+tutorial&format=json&engines=bing%2                                                                                           |\n",
      "|  3 | DuckDuckGo HTML | True  | 5.0     | 1009.0       | 200.0    | python web scraping tutorial |                                                                                                                                                                                                                                                                                                                                  |\n",
      "|  4 | Brave HTML      | False |         |              |          | python web scraping tutorial | Request failed after retries: ContentDecodingError: ('Received response with content-encoding: br, but failed to decode it.', error('BrotliDecoderDecompressStream failed while processing the stream')) | attempts=['verify_true:ContentDecodingError', 'verify_true:ContentDecodingError', 'verify_true:ContentDecodingError'] |\n",
      "|  5 | Bing API        | False |         |              |          | latest LLM research 2025     | Missing required config: bing_api_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                               |\n",
      "|  6 | SerpAPI (Bing)  | False |         |              |          | latest LLM research 2025     | Missing required config: serpapi_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                                |\n",
      "|  7 | SearXNG         | False |         |              |          | latest LLM research 2025     | SearXNG failed: RuntimeError: Request failed after retries: ConnectionError: HTTPSConnectionPool(host='search.bus-hit.me', port=443): Max retries exceeded with url: /search?q=latest+LLM+research+2025&format=json&engines=bing%2Cduc                                                                                           |\n",
      "|  8 | DuckDuckGo HTML | True  | 5.0     | 1206.0       | 200.0    | latest LLM research 2025     |                                                                                                                                                                                                                                                                                                                                  |\n",
      "|  9 | Brave HTML      | False |         |              |          | latest LLM research 2025     | Request failed after retries: ContentDecodingError: ('Received response with content-encoding: br, but failed to decode it.', error('BrotliDecoderDecompressStream failed while processing the stream')) | attempts=['verify_true:ContentDecodingError', 'verify_true:ContentDecodingError', 'verify_true:ContentDecodingError'] |\n",
      "| 10 | Bing API        | False |         |              |          | how to use pandas merge      | Missing required config: bing_api_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                               |\n",
      "| 11 | SerpAPI (Bing)  | False |         |              |          | how to use pandas merge      | Missing required config: serpapi_key. Set env var or edit CONFIG.                                                                                                                                                                                                                                                                |\n",
      "| 12 | SearXNG         | False |         |              |          | how to use pandas merge      | SearXNG failed: RuntimeError: Request failed after retries: ConnectionError: HTTPSConnectionPool(host='search.bus-hit.me', port=443): Max retries exceeded with url: /search?q=how+to+use+pandas+merge&format=json&engines=bing%2Cduck                                                                                           |\n",
      "| 13 | DuckDuckGo HTML | True  | 5.0     | 1055.0       | 200.0    | how to use pandas merge      |                                                                                                                                                                                                                                                                                                                                  |\n",
      "| 14 | Brave HTML      | False |         |              |          | how to use pandas merge      | Request failed after retries: ContentDecodingError: ('Received response with content-encoding: br, but failed to decode it.', error('BrotliDecoderDecompressStream failed while processing the stream')) | attempts=['verify_true:ContentDecodingError', 'verify_true:ContentDecodingError', 'verify_true:ContentDecodingError'] |\n",
      "\n",
      "Samples:\n",
      "\n",
      "== DuckDuckGo HTML | Query: python web scraping tutorial | Count: 5.0 | 1009.0 ms ==\n",
      "1. Python Web Scraping Tutorial - GeeksforGeeks\n",
      "   Webscrapingis the process of extracting data from websites automatically.Pythonis widely used forwebscrapingbecause of its easy syntax and powerful libraries li...\n",
      "   URL: https://www.geeksforgeeks.org/python/python-web-scraping-tutorial/\n",
      "2. Python Web Scraping: Full Tutorial With Examples (2025)\n",
      "   Learn how to scrape data from websites automatically withPython, from the basics to advanced techniques. Thistutorialcoverswebscrapingtools, methods, tips, and ...\n",
      "   URL: https://www.scrapingbee.com/blog/web-scraping-101-with-python/\n",
      "\n",
      "== DuckDuckGo HTML | Query: latest LLM research 2025 | Count: 5.0 | 1206.0 ms ==\n",
      "1. LLM Research Papers: The 2025 List (January to June)\n",
      "   ThelatestinLLMresearchwith a hand-curated, topic-organized list of over 200researchpapers from2025.\n",
      "   URL: https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one\n",
      "2. LLM Research Papers from 2025 You Should Read - Analytics Vidhya\n",
      "   Uncover the topLLMresearchpapers of2025. This guide summarizes significant AI breakthroughs in large language models and their impacts.\n",
      "   URL: https://www.analyticsvidhya.com/blog/2025/06/top-llm-research-papers-of-2025/\n",
      "\n",
      "== DuckDuckGo HTML | Query: how to use pandas merge | Count: 5.0 | 1055.0 ms ==\n",
      "1. Merge, join, concatenate and compare — pandas 2.3.2 documentation\n",
      "   Merge, join, concatenate and compare #pandasprovides various methods for combining and comparing Series or DataFrame. concat():Mergemultiple Series or DataFrame...\n",
      "   URL: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
      "2. Joining two Pandas DataFrames using merge() - GeeksforGeeks\n",
      "   Themerge() function is designed tomergetwo DataFrames based on one or more columns with matching values. The basic idea is to identify columns that contain comm...\n",
      "   URL: https://www.geeksforgeeks.org/python/joining-two-pandas-dataframes-using-merge/\n"
     ]
    }
   ],
   "source": [
    "# Run and report\n",
    "\n",
    "# Optional: quickly check which providers are enabled by config\n",
    "print('Config summary:')\n",
    "print('- bing_api_key set:', bool(CONFIG['bing_api_key']))\n",
    "print('- serpapi_key set:', bool(CONFIG['serpapi_key']))\n",
    "print('- searxng_base:', CONFIG.get('searxng_base', 'http://172.20.221.97:8080'))\n",
    "\n",
    "# Execute benchmarks\n",
    "results_df = run_benchmarks(TEST_QUERIES)\n",
    "\n",
    "# Summary table\n",
    "summary_cols = ['provider', 'ok', 'count', 'latency_ms', 'status', 'query', 'error']\n",
    "summary = results_df.reindex(columns=[c for c in summary_cols if c in results_df.columns])\n",
    "print('\\nSummary:')\n",
    "print(tabulate(summary.fillna(''), headers='keys', tablefmt='github'))\n",
    "\n",
    "# Show sample results for successful runs\n",
    "print('\\nSamples:')\n",
    "for _, row in results_df.iterrows():\n",
    "    if row.get('ok') and row.get('sample'):\n",
    "        print(f\"\\n== {row['provider']} | Query: {row['query']} | Count: {row['count']} | {row.get('latency_ms', '')} ms ==\")\n",
    "        for i, item in enumerate(row['sample'], 1):\n",
    "            print(f\"{i}. {item.get('title','')}\")\n",
    "            if item.get('snippet'):\n",
    "                print(f\"   {truncate(item['snippet'], 160)}\")\n",
    "            if item.get('url'):\n",
    "                print(f\"   URL: {item['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
