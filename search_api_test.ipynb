{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Search API Test Notebook\n",
        "\n",
        "This self-contained notebook tests multiple web search providers and reports latency, status, and top results.\n",
        "\n",
        "Providers included:\n",
        "- Bing Web Search API (Azure Cognitive Services)\n",
        "- SerpAPI (engine=bing)\n",
        "- SearXNG (public or self-hosted)\n",
        "- DuckDuckGo HTML (no-JS HTML endpoint)\n",
        "- Brave HTML\n",
        "\n",
        "Instructions:\n",
        "- Set API keys/endpoints in the Config cell below (or via environment variables).\n",
        "- Run the cells top-to-bottom. The notebook installs missing dependencies at runtime.\n",
        "- Results include a summary table and sample results per provider.\n",
        "\n",
        "Notes:\n",
        "- Use responsibly and comply with each provider's Terms of Service.\n",
        "- HTML providers may change markup; simple parsers here are best-effort.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime setup: imports and on-the-fly installs\n",
        "import sys, subprocess, json, os, time, random\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# Lazy installer\n",
        "\n",
        "def ensure(package: str, import_name: Optional[str] = None):\n",
        "    try:\n",
        "        __import__(import_name or package)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "# Ensure dependencies used by HTML parsers and table rendering\n",
        "for pkg, imp in [\n",
        "    ('requests', 'requests'),\n",
        "    ('beautifulsoup4', 'bs4'),\n",
        "    ('pandas', 'pandas'),\n",
        "    ('tabulate', 'tabulate'),\n",
        "]:\n",
        "    ensure(pkg, imp)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Utility: timing\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start = time.time()\n",
        "        return self\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        self.end = time.time()\n",
        "        self.ms = int((self.end - self.start) * 1000)\n",
        "\n",
        "# Normalize result schema\n",
        "Result = Dict[str, str]\n",
        "\n",
        "\n",
        "def truncate(text: str, n: int = 200) -> str:\n",
        "    if not text:\n",
        "        return ''\n",
        "    return text if len(text) <= n else text[:n] + '...'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config: API keys and endpoints\n",
        "CONFIG = {\n",
        "    # Bing Web Search API (Azure)\n",
        "    # Docs: https://learn.microsoft.com/azure/cognitive-services/bing-web-search/\n",
        "    'bing_api_key': os.getenv('BING_API_KEY', ''),\n",
        "    'bing_endpoint': os.getenv('BING_ENDPOINT', 'https://api.bing.microsoft.com/v7.0/search'),\n",
        "\n",
        "    # SerpAPI (Bing engine)\n",
        "    # Docs: https://serpapi.com/\n",
        "    'serpapi_key': os.getenv('SERPAPI_KEY', ''),\n",
        "\n",
        "    # SearXNG public/self-hosted endpoint\n",
        "    # Example public: https://search.bus-hit.me\n",
        "    'searxng_base': os.getenv('SEARXNG_BASE', 'https://search.bus-hit.me'),\n",
        "\n",
        "    # HTML providers config\n",
        "    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'timeout': 15,\n",
        "}\n",
        "\n",
        "# Basic validation helper\n",
        "\n",
        "def require(value: str, name: str) -> None:\n",
        "    if not value:\n",
        "        raise ValueError(f\"Missing required config: {name}. Set env var or edit CONFIG.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connectivity diagnostics (optional)\n",
        "import socket\n",
        "\n",
        "print('— Connectivity checks —')\n",
        "try:\n",
        "    r = requests.get('https://example.com', timeout=10)\n",
        "    print('example.com:', r.status_code)\n",
        "except Exception as e:\n",
        "    print('example.com failed:', type(e).__name__, str(e)[:200])\n",
        "\n",
        "try:\n",
        "    ip = requests.get('https://api.ipify.org?format=json', timeout=10)\n",
        "    print('Public IP:', ip.json().get('ip'))\n",
        "except Exception as e:\n",
        "    print('Public IP check failed:', type(e).__name__, str(e)[:200])\n",
        "\n",
        "print('HTTP_PROXY:', os.getenv('HTTP_PROXY') or os.getenv('http_proxy'))\n",
        "print('HTTPS_PROXY:', os.getenv('HTTPS_PROXY') or os.getenv('https_proxy'))\n",
        "try:\n",
        "    print('Hostname:', socket.gethostname())\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provider implementations\n",
        "\n",
        "headers_common = {\n",
        "    'User-Agent': CONFIG['user_agent'],\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "}\n",
        "\n",
        "\n",
        "def search_bing_api(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    require(CONFIG['bing_api_key'], 'bing_api_key')\n",
        "    url = CONFIG['bing_endpoint']\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'count': max_results,\n",
        "        'mkt': 'en-US',\n",
        "        'responseFilter': 'Webpages'\n",
        "    }\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': CONFIG['bing_api_key']\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, headers=headers, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for w in (data.get('webPages', {}) or {}).get('value', []):\n",
        "        items.append({\n",
        "            'title': w.get('name', ''),\n",
        "            'snippet': truncate(w.get('snippet', ''), 200),\n",
        "            'url': w.get('url', ''),\n",
        "            'source': 'Bing API'\n",
        "        })\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_serpapi_bing(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    require(CONFIG['serpapi_key'], 'serpapi_key')\n",
        "    url = 'https://serpapi.com/search.json'\n",
        "    params = {\n",
        "        'engine': 'bing',\n",
        "        'q': query,\n",
        "        'num': max_results,\n",
        "        'api_key': CONFIG['serpapi_key']\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for res in data.get('organic_results', [])[:max_results]:\n",
        "        items.append({\n",
        "            'title': res.get('title', ''),\n",
        "            'snippet': truncate(res.get('snippet', ''), 200),\n",
        "            'url': res.get('link', ''),\n",
        "            'source': 'SerpAPI (Bing)'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    base = CONFIG['searxng_base'].rstrip('/')\n",
        "    url = f\"{base}/search\"\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'format': 'json',\n",
        "        'engines': 'bing,duckduckgo',\n",
        "        'language': 'en'\n",
        "    }\n",
        "    with Timer() as t:\n",
        "        r = requests.get(url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "    items = []\n",
        "    for res in data.get('results', [])[:max_results]:\n",
        "        items.append({\n",
        "            'title': res.get('title', ''),\n",
        "            'snippet': truncate(res.get('content', ''), 200),\n",
        "            'url': res.get('url', ''),\n",
        "            'source': 'SearXNG'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    url = 'https://html.duckduckgo.com/html/'\n",
        "    payload = {'q': query}\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    with Timer() as t:\n",
        "        r = session.post(url, data=payload, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    for element in soup.find_all('div', class_='result')[:max_results]:\n",
        "        a = element.find('a', class_='result__a')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        snippet_elem = element.find('a', class_='result__snippet')\n",
        "        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
        "        if not title or not href:\n",
        "            continue\n",
        "        items.append({\n",
        "            'title': title,\n",
        "            'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
        "            'url': href,\n",
        "            'source': 'DuckDuckGo (HTML)'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    from urllib.parse import quote_plus\n",
        "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    with Timer() as t:\n",
        "        r = session.get(url, timeout=CONFIG['timeout'])\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    # Brave changes often; attempt multiple selectors\n",
        "    # Primary: div.snippet with a.result-header\n",
        "    for element in soup.find_all('div', class_='snippet'):\n",
        "        a = element.find('a', class_='result-header')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        desc = ''\n",
        "        p = element.find('p', class_='snippet-description')\n",
        "        if p:\n",
        "            desc = p.get_text(strip=True)\n",
        "        if title and href and href.startswith('http'):\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
        "                'url': href,\n",
        "                'source': 'Brave (HTML)'\n",
        "            })\n",
        "        if len(items) >= max_results:\n",
        "            break\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark harness\n",
        "\n",
        "PROVIDERS = [\n",
        "    ('Bing API', search_bing_api),\n",
        "    ('SerpAPI (Bing)', search_serpapi_bing),\n",
        "    ('SearXNG', search_searxng),\n",
        "    ('DuckDuckGo HTML', search_duckduckgo_html),\n",
        "    ('Brave HTML', search_brave_html),\n",
        "]\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    'python web scraping tutorial',\n",
        "    'latest LLM research 2025',\n",
        "    'how to use pandas merge',\n",
        "]\n",
        "\n",
        "MAX_RESULTS = 5\n",
        "\n",
        "\n",
        "def run_single(provider_name: str, fn, query: str, max_results: int = MAX_RESULTS) -> Dict[str, Any]:\n",
        "    try:\n",
        "        results, meta = fn(query, max_results=max_results)\n",
        "        return {\n",
        "            'provider': provider_name,\n",
        "            'query': query,\n",
        "            'ok': True,\n",
        "            'count': len(results),\n",
        "            'latency_ms': meta.get('latency_ms', None),\n",
        "            'status': meta.get('status', None),\n",
        "            'sample': results[:2] if results else []\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'provider': provider_name,\n",
        "            'query': query,\n",
        "            'ok': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "def run_benchmarks(queries: List[str] = TEST_QUERIES) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for q in queries:\n",
        "        for name, fn in PROVIDERS:\n",
        "            rows.append(run_single(name, fn, q))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SearXNG: SSL fallback, proxy support, and endpoint rotation\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SEARXNG_ENDPOINTS = [\n",
        "    CONFIG['searxng_base'],\n",
        "    'https://searx.be',\n",
        "    'https://searx.tiekoetter.com',\n",
        "    'https://searxng.site',\n",
        "]\n",
        "\n",
        "PROXY_ENV = {\n",
        "    'http': os.getenv('HTTP_PROXY') or os.getenv('http_proxy') or None,\n",
        "    'https': os.getenv('HTTPS_PROXY') or os.getenv('https_proxy') or None,\n",
        "}\n",
        "\n",
        "# Replace search_searxng with a hardened version\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    headers = dict(headers_common)\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    # Try multiple endpoints, handle SSL errors, and optionally disable verification as last resort\n",
        "    last_exc = None\n",
        "    for idx, base in enumerate([e for e in SEARXNG_ENDPOINTS if e]):\n",
        "        base = base.rstrip('/')\n",
        "        url = f\"{base}/search\"\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'format': 'json',\n",
        "            'engines': 'bing,duckduckgo',\n",
        "            'language': 'en'\n",
        "        }\n",
        "\n",
        "        # Attempt 1: normal GET with proxies if set\n",
        "        try:\n",
        "            with Timer() as t:\n",
        "                r = session.get(url, params=params, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "                r.raise_for_status()\n",
        "                data = r.json()\n",
        "            items = []\n",
        "            for res in data.get('results', [])[:max_results]:\n",
        "                items.append({\n",
        "                    'title': res.get('title', ''),\n",
        "                    'snippet': truncate(res.get('content', ''), 200),\n",
        "                    'url': res.get('url', ''),\n",
        "                    'source': f\"SearXNG ({urlparse(base).netloc})\"\n",
        "                })\n",
        "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "        except requests.exceptions.SSLError as e:\n",
        "            last_exc = e\n",
        "            # Attempt 2: allow self-signed/invalid certs (verification off) for lab environments\n",
        "            try:\n",
        "                with Timer() as t:\n",
        "                    r = session.get(url, params=params, timeout=CONFIG['timeout'], verify=False, proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "                    r.raise_for_status()\n",
        "                    data = r.json()\n",
        "                items = []\n",
        "                for res in data.get('results', [])[:max_results]:\n",
        "                    items.append({\n",
        "                        'title': res.get('title', ''),\n",
        "                        'snippet': truncate(res.get('content', ''), 200),\n",
        "                        'url': res.get('url', ''),\n",
        "                        'source': f\"SearXNG (insecure) ({urlparse(base).netloc})\"\n",
        "                    })\n",
        "                return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "            except Exception as e2:\n",
        "                last_exc = e2\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            continue\n",
        "\n",
        "    raise RuntimeError(f\"SearXNG failed across endpoints: {type(last_exc).__name__}: {str(last_exc)[:200]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HTML providers: SSL fallback and proxy support\n",
        "\n",
        "PROXY_ENV = {\n",
        "    'http': os.getenv('HTTP_PROXY') or os.getenv('http_proxy') or None,\n",
        "    'https': os.getenv('HTTPS_PROXY') or os.getenv('https_proxy') or None,\n",
        "}\n",
        "\n",
        "ALLOW_INSECURE_SSL = bool(os.getenv('ALLOW_INSECURE_SSL', '0') in ['1', 'true', 'True'])\n",
        "\n",
        "# Wrap the existing HTML functions with hardened versions\n",
        "\n",
        "_original_ddg = search_duckduckgo_html\n",
        "_original_brave = search_brave_html\n",
        "\n",
        "\n",
        "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    url = 'https://html.duckduckgo.com/html/'\n",
        "    payload = {'q': query}\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    # Attempt 1: normal POST\n",
        "    try:\n",
        "        with Timer() as t:\n",
        "            r = session.post(url, data=payload, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        items: List[Result] = []\n",
        "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
        "            a = element.find('a', class_='result__a')\n",
        "            if not a:\n",
        "                continue\n",
        "            title = a.get_text(strip=True) or ''\n",
        "            href = a.get('href', '')\n",
        "            snippet_elem = element.find('a', class_='result__snippet')\n",
        "            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
        "            if not title or not href:\n",
        "                continue\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
        "                'url': href,\n",
        "                'source': 'DuckDuckGo (HTML)'\n",
        "            })\n",
        "        return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "    except requests.exceptions.SSLError as e:\n",
        "        if not ALLOW_INSECURE_SSL:\n",
        "            raise\n",
        "        # Attempt 2: insecure SSL for lab envs\n",
        "        with Timer() as t:\n",
        "            r = session.post(url, data=payload, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None, verify=False)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        items: List[Result] = []\n",
        "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
        "            a = element.find('a', class_='result__a')\n",
        "            if not a:\n",
        "                continue\n",
        "            title = a.get_text(strip=True) or ''\n",
        "            href = a.get('href', '')\n",
        "            snippet_elem = element.find('a', class_='result__snippet')\n",
        "            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
        "            if not title or not href:\n",
        "                continue\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
        "                'url': href,\n",
        "                'source': 'DuckDuckGo (HTML, insecure)'\n",
        "            })\n",
        "        return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    from urllib.parse import quote_plus\n",
        "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers_common)\n",
        "    # Attempt 1: normal GET\n",
        "    try:\n",
        "        with Timer() as t:\n",
        "            r = session.get(url, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    except requests.exceptions.SSLError as e:\n",
        "        if not ALLOW_INSECURE_SSL:\n",
        "            raise\n",
        "        with Timer() as t:\n",
        "            r = session.get(url, timeout=CONFIG['timeout'], proxies=PROXY_ENV if any(PROXY_ENV.values()) else None, verify=False)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    for element in soup.find_all('div', class_='snippet'):\n",
        "        a = element.find('a', class_='result-header')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        desc = ''\n",
        "        p = element.find('p', class_='snippet-description')\n",
        "        if p:\n",
        "            desc = p.get_text(strip=True)\n",
        "        if title and href and href.startswith('http'):\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
        "                'url': href,\n",
        "                'source': 'Brave (HTML, insecure)' if 'verify=False' in str(r.request.__dict__) else 'Brave (HTML)'\n",
        "            })\n",
        "        if len(items) >= max_results:\n",
        "            break\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unified request helper: proxies, CA bundle, retries, insecure fallback\n",
        "import urllib3\n",
        "from requests.exceptions import SSLError, ProxyError, ConnectionError, ReadTimeout\n",
        "\n",
        "# Optional config via env or direct edits here\n",
        "PROXY_URL = os.getenv('PROXY_URL', '').strip()  # e.g., http://user:pass@host:port\n",
        "PROXY_USERNAME = os.getenv('PROXY_USERNAME', '').strip()\n",
        "PROXY_PASSWORD = os.getenv('PROXY_PASSWORD', '').strip()\n",
        "CA_BUNDLE_PATH = os.getenv('CA_BUNDLE_PATH', '').strip()  # e.g., C:\\certs\\corp_ca.pem\n",
        "RETRIES = int(os.getenv('REQUEST_RETRIES', '2'))\n",
        "BACKOFF = float(os.getenv('REQUEST_BACKOFF', '0.7'))\n",
        "\n",
        "if ALLOW_INSECURE_SSL:\n",
        "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "\n",
        "def build_proxies() -> Optional[Dict[str, str]]:\n",
        "    # Prefer explicit PROXY_URL if provided; else environment proxies\n",
        "    if PROXY_URL:\n",
        "        return {'http': PROXY_URL, 'https': PROXY_URL}\n",
        "    if any(PROXY_ENV.values()):\n",
        "        return PROXY_ENV\n",
        "    # Support basic auth if host provided separately (rare); construct URL\n",
        "    if os.getenv('PROXY_HOST') and os.getenv('PROXY_PORT'):\n",
        "        host = os.getenv('PROXY_HOST').strip()\n",
        "        port = os.getenv('PROXY_PORT').strip()\n",
        "        user = PROXY_USERNAME or os.getenv('PROXY_USERNAME', '').strip()\n",
        "        pwd = PROXY_PASSWORD or os.getenv('PROXY_PASSWORD', '').strip()\n",
        "        if user and pwd:\n",
        "            return {\n",
        "                'http': f'http://{user}:{pwd}@{host}:{port}',\n",
        "                'https': f'http://{user}:{pwd}@{host}:{port}'\n",
        "            }\n",
        "        return {'http': f'http://{host}:{port}', 'https': f'http://{host}:{port}'}\n",
        "    return None\n",
        "\n",
        "\n",
        "PROXIES = build_proxies()\n",
        "\n",
        "\n",
        "def request_with_fallback(method: str, url: str, *, params=None, data=None, headers=None, timeout=None) -> requests.Response:\n",
        "    last_exc = None\n",
        "    attempts = []\n",
        "\n",
        "    # Strategy order: explicit CA bundle -> default verify -> insecure if allowed\n",
        "    strategies = []\n",
        "    if CA_BUNDLE_PATH:\n",
        "        strategies.append(('verify_path', CA_BUNDLE_PATH))\n",
        "    strategies.append(('verify_true', True))\n",
        "    if ALLOW_INSECURE_SSL:\n",
        "        strategies.append(('verify_false', False))\n",
        "\n",
        "    for attempt in range(RETRIES + 1):\n",
        "        for label, verify in strategies:\n",
        "            try:\n",
        "                resp = requests.request(\n",
        "                    method,\n",
        "                    url,\n",
        "                    params=params,\n",
        "                    data=data,\n",
        "                    headers=headers,\n",
        "                    timeout=timeout or CONFIG['timeout'],\n",
        "                    proxies=PROXIES,\n",
        "                    verify=verify\n",
        "                )\n",
        "                resp.raise_for_status()\n",
        "                return resp\n",
        "            except (SSLError, ProxyError, ConnectionError, ReadTimeout) as e:\n",
        "                last_exc = e\n",
        "                attempts.append(f\"{label}:{type(e).__name__}\")\n",
        "                time.sleep(BACKOFF * (attempt + 1))\n",
        "            except Exception as e:\n",
        "                last_exc = e\n",
        "                attempts.append(f\"{label}:{type(e).__name__}\")\n",
        "                time.sleep(BACKOFF * (attempt + 1))\n",
        "    raise RuntimeError(f\"Request failed after retries: {type(last_exc).__name__}: {str(last_exc)[:200]} | attempts={attempts}\")\n",
        "\n",
        "\n",
        "print('— Network wrapper ready —')\n",
        "print('PROXIES:', PROXIES)\n",
        "print('CA_BUNDLE_PATH:', CA_BUNDLE_PATH or '(default trust store)')\n",
        "print('ALLOW_INSECURE_SSL:', ALLOW_INSECURE_SSL)\n",
        "print('RETRIES:', RETRIES, 'BACKOFF:', BACKOFF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rebind providers to use the unified request helper\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    from urllib.parse import urlparse\n",
        "    last_exc = None\n",
        "    for base in [e for e in SEARXNG_ENDPOINTS if e]:\n",
        "        base = base.rstrip('/')\n",
        "        url = f\"{base}/search\"\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'format': 'json',\n",
        "            'engines': 'bing,duckduckgo',\n",
        "            'language': 'en'\n",
        "        }\n",
        "        try:\n",
        "            with Timer() as t:\n",
        "                r = request_with_fallback('GET', url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "                data = r.json()\n",
        "            items = []\n",
        "            for res in data.get('results', [])[:max_results]:\n",
        "                items.append({\n",
        "                    'title': res.get('title', ''),\n",
        "                    'snippet': truncate(res.get('content', ''), 200),\n",
        "                    'url': res.get('url', ''),\n",
        "                    'source': f\"SearXNG ({urlparse(base).netloc})\"\n",
        "                })\n",
        "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            continue\n",
        "    raise RuntimeError(f\"SearXNG failed: {type(last_exc).__name__}: {str(last_exc)[:200]}\")\n",
        "\n",
        "\n",
        "def search_duckduckgo_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    url = 'https://html.duckduckgo.com/html/'\n",
        "    payload = {'q': query}\n",
        "    with Timer() as t:\n",
        "        r = request_with_fallback('POST', url, data=payload, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    for element in soup.find_all('div', class_='result')[:max_results]:\n",
        "        a = element.find('a', class_='result__a')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        snippet_elem = element.find('a', class_='result__snippet')\n",
        "        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
        "        if not title or not href:\n",
        "            continue\n",
        "        items.append({\n",
        "            'title': title,\n",
        "            'snippet': truncate(snippet, 200) if snippet else 'DuckDuckGo result',\n",
        "            'url': href,\n",
        "            'source': 'DuckDuckGo (HTML)'\n",
        "        })\n",
        "    return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "\n",
        "def search_brave_html(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    from urllib.parse import quote_plus\n",
        "    url = f\"https://search.brave.com/search?q={quote_plus(query)}\"\n",
        "    with Timer() as t:\n",
        "        r = request_with_fallback('GET', url, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    items: List[Result] = []\n",
        "    for element in soup.find_all('div', class_='snippet'):\n",
        "        a = element.find('a', class_='result-header')\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True) or ''\n",
        "        href = a.get('href', '')\n",
        "        desc = ''\n",
        "        p = element.find('p', class_='snippet-description')\n",
        "        if p:\n",
        "            desc = p.get_text(strip=True)\n",
        "        if title and href and href.startswith('http'):\n",
        "            items.append({\n",
        "                'title': title,\n",
        "                'snippet': truncate(desc, 200) if desc else 'Brave result',\n",
        "                'url': href,\n",
        "                'source': 'Brave (HTML)'\n",
        "            })\n",
        "        if len(items) >= max_results:\n",
        "            break\n",
        "    return items[:max_results], {'latency_ms': t.ms, 'status': r.status_code}\n",
        "\n",
        "# Update provider list to use the rebound functions\n",
        "PROVIDERS = [\n",
        "    ('Bing API', search_bing_api),\n",
        "    ('SerpAPI (Bing)', search_serpapi_bing),\n",
        "    ('SearXNG', search_searxng),\n",
        "    ('DuckDuckGo HTML', search_duckduckgo_html),\n",
        "    ('Brave HTML', search_brave_html),\n",
        "]\n",
        "\n",
        "print('Providers rebound to robust network wrapper')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proxy/SSL diagnostics (company network)\n",
        "import ssl\n",
        "import certifi\n",
        "\n",
        "print('— Proxy/SSL Diagnostics —')\n",
        "print('HTTP_PROXY:', os.getenv('HTTP_PROXY') or os.getenv('http_proxy'))\n",
        "print('HTTPS_PROXY:', os.getenv('HTTPS_PROXY') or os.getenv('https_proxy'))\n",
        "print('PROXY_URL:', os.getenv('PROXY_URL'))\n",
        "print('PROXY_HOST:', os.getenv('PROXY_HOST'))\n",
        "print('PROXY_PORT:', os.getenv('PROXY_PORT'))\n",
        "print('CA_BUNDLE_PATH:', os.getenv('CA_BUNDLE_PATH'))\n",
        "print('ALLOW_INSECURE_SSL:', os.getenv('ALLOW_INSECURE_SSL'))\n",
        "\n",
        "# Test DNS and TCP via proxies if configured\n",
        "TEST_URLS = [\n",
        "    'https://example.com',\n",
        "    CONFIG.get('searxng_base', 'https://search.bus-hit.me').rstrip('/') + '/search'\n",
        "]\n",
        "\n",
        "for u in TEST_URLS:\n",
        "    try:\n",
        "        with Timer() as t:\n",
        "            r = request_with_fallback('GET', u, headers={'User-Agent': CONFIG['user_agent']}, timeout=10)\n",
        "        print(f'OK {u} in {t.ms} ms - status {r.status_code}')\n",
        "    except Exception as e:\n",
        "        print(f'FAIL {u}:', type(e).__name__, str(e)[:300])\n",
        "\n",
        "# Show default cert store in use\n",
        "try:\n",
        "    print('certifi.where():', certifi.where())\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    ctx = ssl.create_default_context(cafile=os.getenv('CA_BUNDLE_PATH') or None)\n",
        "    print('SSL context created. Custom CA used:', bool(os.getenv('CA_BUNDLE_PATH')))\n",
        "except Exception as e:\n",
        "    print('SSL context error:', type(e).__name__, str(e)[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accessing SearXNG directly via URL\n",
        "\n",
        "You can open a SearXNG instance in your browser and run a search by navigating to:\n",
        "\n",
        "- Base URL (homepage):\n",
        "  - `https://search.bus-hit.me/`\n",
        "- JSON API endpoint (browser-friendly for quick checks):\n",
        "  - `https://search.bus-hit.me/search?q=python&format=json`\n",
        "- HTML results page:\n",
        "  - `https://search.bus-hit.me/search?q=python`\n",
        "\n",
        "Replace `search.bus-hit.me` with your preferred or company-hosted SearXNG instance.\n",
        "\n",
        "Common public instances (availability varies):\n",
        "- `https://searx.be`\n",
        "- `https://searx.tiekoetter.com`\n",
        "- `https://searxng.site`\n",
        "\n",
        "Tip: If you are behind a corporate proxy with a custom CA, your browser may trust the proxy automatically. If you want Python to trust the same CA, export the CA bundle path and rerun the notebook:\n",
        "- Windows PowerShell:\n",
        "  - `$env:CA_BUNDLE_PATH = \"C:\\\\path\\\\to\\\\corp_ca.pem\"`\n",
        "- CMD:\n",
        "  - `set CA_BUNDLE_PATH=C:\\path\\to\\corp_ca.pem`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SearXNG endpoint health and rotation (handle 403 bans)\n",
        "import time as _time\n",
        "from urllib.parse import urlparse as _urlparse\n",
        "from requests.exceptions import HTTPError as _HTTPError\n",
        "\n",
        "SEARXNG_HEALTH = globals().get('SEARXNG_HEALTH', {})  # persist across runs in session\n",
        "SEARXNG_BAN_SECONDS = int(os.getenv('SEARXNG_BAN_SECONDS', '3600'))  # 1 hour default\n",
        "SEARXNG_FAVOR_HOSTS = {'searx.tiekoetter.com': 2}  # weight boost for known-good\n",
        "\n",
        "\n",
        "def _now() -> float:\n",
        "    return _time.time()\n",
        "\n",
        "\n",
        "def _host_of(base: str) -> str:\n",
        "    try:\n",
        "        return _urlparse(base).netloc\n",
        "    except Exception:\n",
        "        return base\n",
        "\n",
        "\n",
        "def get_searxng_candidates() -> list:\n",
        "    # Build weighted list with health and bans\n",
        "    candidates = []\n",
        "    now = _now()\n",
        "    for base in [e for e in SEARXNG_ENDPOINTS if e]:\n",
        "        host = _host_of(base)\n",
        "        h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
        "        banned = h.get('banned_until', 0) > now\n",
        "        weight = h.get('ok', 0) - h.get('fail', 0)\n",
        "        weight += SEARXNG_FAVOR_HOSTS.get(host, 0)\n",
        "        candidates.append((banned, -weight, base))\n",
        "    # Not banned first, higher weight first\n",
        "    candidates.sort()\n",
        "    # Return ordered bases\n",
        "    ordered = [base for banned, _w, base in candidates if not banned]\n",
        "    # Append banned ones at the end (last resort if bans expire inside same run)\n",
        "    ordered += [base for banned, _w, base in candidates if banned]\n",
        "    return ordered\n",
        "\n",
        "\n",
        "def search_searxng(query: str, max_results: int = 5) -> Tuple[List[Result], Dict[str, Any]]:\n",
        "    ordered = get_searxng_candidates()\n",
        "    last_exc = None\n",
        "    for base in ordered:\n",
        "        base = base.rstrip('/')\n",
        "        host = _host_of(base)\n",
        "        url = f\"{base}/search\"\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'format': 'json',\n",
        "            'engines': 'bing,duckduckgo',\n",
        "            'language': 'en'\n",
        "        }\n",
        "        try:\n",
        "            with Timer() as t:\n",
        "                r = request_with_fallback('GET', url, params=params, headers=headers_common, timeout=CONFIG['timeout'])\n",
        "                data = r.json()\n",
        "            items = []\n",
        "            for res in data.get('results', [])[:max_results]:\n",
        "                items.append({\n",
        "                    'title': res.get('title', ''),\n",
        "                    'snippet': truncate(res.get('content', ''), 200),\n",
        "                    'url': res.get('url', ''),\n",
        "                    'source': f\"SearXNG ({host})\"\n",
        "                })\n",
        "            # mark success\n",
        "            h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
        "            h['ok'] = h.get('ok', 0) + 1\n",
        "            h['last_ok_at'] = _now()\n",
        "            h['banned_until'] = 0\n",
        "            SEARXNG_HEALTH[host] = h\n",
        "            return items, {'latency_ms': t.ms, 'status': r.status_code}\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            emsg = str(e)\n",
        "            # Detect 403 and ban temporarily\n",
        "            if '403' in emsg or 'Forbidden' in emsg:\n",
        "                h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
        "                h['fail'] = h.get('fail', 0) + 1\n",
        "                h['banned_until'] = _now() + SEARXNG_BAN_SECONDS\n",
        "                h['last_status'] = 403\n",
        "                SEARXNG_HEALTH[host] = h\n",
        "            else:\n",
        "                h = SEARXNG_HEALTH.get(host, {'ok': 0, 'fail': 0, 'banned_until': 0})\n",
        "                h['fail'] = h.get('fail', 0) + 1\n",
        "                h['last_status'] = getattr(getattr(e, 'response', None), 'status_code', None)\n",
        "                SEARXNG_HEALTH[host] = h\n",
        "            continue\n",
        "    raise RuntimeError(f\"SearXNG all endpoints failed. Last error: {type(last_exc).__name__}: {str(last_exc)[:200]}. Health={SEARXNG_HEALTH}\")\n",
        "\n",
        "print('SearXNG rotation enabled. Ordered candidates:', get_searxng_candidates())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run and report\n",
        "\n",
        "# Optional: quickly check which providers are enabled by config\n",
        "print('Config summary:')\n",
        "print('- bing_api_key set:', bool(CONFIG['bing_api_key']))\n",
        "print('- serpapi_key set:', bool(CONFIG['serpapi_key']))\n",
        "print('- searxng_base:', CONFIG['searxng_base'])\n",
        "\n",
        "# Execute benchmarks\n",
        "results_df = run_benchmarks(TEST_QUERIES)\n",
        "\n",
        "# Summary table\n",
        "summary_cols = ['provider', 'ok', 'count', 'latency_ms', 'status', 'query', 'error']\n",
        "summary = results_df.reindex(columns=[c for c in summary_cols if c in results_df.columns])\n",
        "print('\\nSummary:')\n",
        "print(tabulate(summary.fillna(''), headers='keys', tablefmt='github'))\n",
        "\n",
        "# Show sample results for successful runs\n",
        "print('\\nSamples:')\n",
        "for _, row in results_df.iterrows():\n",
        "    if row.get('ok') and row.get('sample'):\n",
        "        print(f\"\\n== {row['provider']} | Query: {row['query']} | Count: {row['count']} | {row.get('latency_ms', '')} ms ==\")\n",
        "        for i, item in enumerate(row['sample'], 1):\n",
        "            print(f\"{i}. {item.get('title','')}\")\n",
        "            if item.get('snippet'):\n",
        "                print(f\"   {truncate(item['snippet'], 160)}\")\n",
        "            if item.get('url'):\n",
        "                print(f\"   URL: {item['url']}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
